
--- ruby version
# to run ternding from ruby 
$ cd trending
$ source rc
$ 2gram_trends.test.rb

--- pig version

see run.sh

TODOS:
 - cull values with mean under some small threshold
 - run through nltk for tokenization first before running ngrammer 
  - first as a single step as part of split into chunks
  - then as first step on chunk before ngramming
 - move the split into chunks responsibility into pig too
 - how to facet trends with an arbitrary key?
  - eg trends per forum
 - link back from trend to document (ie build an index during early chunk phase)

---- for the thorntree hackday version

$ source rc

# need to sort by time, not id, since that's how we bucket into the timeslots
$ zcat ../tt/tt.posts.tsv.gz | head -n1000 | sort -t$'\t' -k2 -n | ../tt/extract_body.rb | split_into_chunks.rb

$ run_iteration.rb 0


################################################
# notes for cloudera distro

see http://archive.cloudera.com/docs/_basic_usage.html for env vars and stuff

$ hadoop-ec2 launch-cluster my-hadoop-cluster 1
$ export MASTER=ec2-75-101-203-200.compute-1.amazonaws.com
$ scp -i /home/mat/dev/ec2/security/id_rsa-gsg-keypair ~/dev/ec2/.s3cfg ~/dev/trending/pig/{trending.pig,ngram.rb} root@$MASTER:

% run on master
$ ssh_ec2 $MASTER
# setup values in /usr/lib/hadoop-0.20/conf/hadoop-site.xml
alias hfs='hadoop fs'
wget http://sourceforge.net/projects/s3tools/files/s3cmd/0.9.9.91/s3cmd-0.9.9.91.tar.gz/download
tar zxf s3cmd-0.9.9.91.tar.gz
export PATH=$PATH:~/s3cmd-0.9.9.91
mkdir -p trending/{chunks,model,fft,trending}
touch trending/model/000
cp ngram.rb trending
hfs -copyFromLocal trending/ trending
hfs -cp s3n://matpalm/trending/run1/chunks/* trending/chunks/ # todo: as mr job as described at end of hadoop book





################################################3
# notes for aws emr

$ elastic-mapreduce --create --name test_by3 --num-instances 3 --pig-script --step-name "11_solo" --args -p,piggybankjar=file:/home/hadoop/lib/pig/piggybank.jar --args -p,root_path=s3n://matpalm/trending/run1 --args -p,input=005 --args -p,output=006 --args s3n://matpalm/trending/run1/scripts/trending.pig

$ elastic-mapreduce --jobflow j-240DHRM9K5YEP --pig-script --step-name "10_003" --args -p,piggybankjar=file:/home/hadoop/lib/pig/piggybank.jar --args -p,root_path=s3n://matpalm/trending/run1 --args -p,input=006 --args -p,output=007 --args s3n://matpalm/trending/run1/scripts/trending.pig

$ elastic-mapreduce --create --name 'pigflowtest2' --num-instances 1 --json pigflow.json --alive

        [ 
          { 
            "Name": "setup", 
            "ActionOnFailure": "CANCEL_AND_WAIT", 
            "HadoopJarStep": { 
              "Jar": "s3://us-east-1.elasticmapreduce/libs/script-runner/script-runner.jar",
              "Args": [ 
                 "s3://us-east-1.elasticmapreduce/libs/pig/0.3/fetch"
              ] 
            } 
          },
          { 
            "Name": "process 009", 
            "ActionOnFailure": "CANCEL_AND_WAIT", 
            "HadoopJarStep": { 
              "Jar": "/home/hadoop/lib/pig/pig-0.3-amzn.jar",
              "Args": [ 
                 "-p,piggybankjar=file:/home/hadoop/lib/pig/piggybank.jar", 
                 "-p,root_path=s3n://matpalm/trending/run1", 
		 "-p,input=009",
		 "-p,output=010",
                 "s3n://matpalm/trending/run1/scripts/trending.pig"
              ] 
            } 
          }
        ] 

and for none interactive (ie no --alive) we can run TERMINATE_JOB_FLOW instead CANCEL_AND_WAIT 
