A Force - Crystal Dawn (Original Mix)


--- ruby version
# to run ternding from ruby 
$ cd trending
$ source rc
$ 2gram_trends.test.rb

--- pig version

# regenerate chunks; each corresponds to an hours worth of tweets
$ cd trending/pig
$ source rc
$ zcat /data/twitter/gardenhose/cheese.20100*gz | extract_date_and_text.rb 2>/dev/null | split_into_chunks.rb

# to run trending from pig (wip)
$ bootstrap_dir.sh blah_dir

# and then generate a runmany script
$ gen_run_pig_version.rb blah_dir > run_pig_version.sh
$ sh run_pig_version.sh

################################################3
# notes for cloudera distro

see http://archive.cloudera.com/docs/_basic_usage.html for env vars and stuff

$ hadoop-ec2 launch-cluster my-hadoop-cluster 1
$ export MASTER=ec2-75-101-203-200.compute-1.amazonaws.com
$ scp -i /home/mat/dev/ec2/security/id_rsa-gsg-keypair ~/dev/ec2/.s3cfg ~/dev/trending/pig/{trending.pig,ngram.rb} root@$MASTER:

% run on master
$ ssh_ec2 $MASTER
# setup values in /usr/lib/hadoop-0.20/conf/hadoop-site.xml
alias hfs='hadoop fs'
wget http://sourceforge.net/projects/s3tools/files/s3cmd/0.9.9.91/s3cmd-0.9.9.91.tar.gz/download
tar zxf s3cmd-0.9.9.91.tar.gz
export PATH=$PATH:~/s3cmd-0.9.9.91
mkdir -p trending/{chunks,model,fft,trending}
touch trending/model/000
cp ngram.rb trending
hfs -copyFromLocal trending/ trending
hfs -cp s3n://matpalm/trending/run1/chunks/* trending/chunks/ # todo: as mr job as described at end of hadoop book





################################################3
# notes for aws emr

$ elastic-mapreduce --create --name test_by3 --num-instances 3 --pig-script --step-name "11_solo" --args -p,piggybankjar=file:/home/hadoop/lib/pig/piggybank.jar --args -p,root_path=s3n://matpalm/trending/run1 --args -p,input=005 --args -p,output=006 --args s3n://matpalm/trending/run1/scripts/trending.pig

$ elastic-mapreduce --jobflow j-240DHRM9K5YEP --pig-script --step-name "10_003" --args -p,piggybankjar=file:/home/hadoop/lib/pig/piggybank.jar --args -p,root_path=s3n://matpalm/trending/run1 --args -p,input=006 --args -p,output=007 --args s3n://matpalm/trending/run1/scripts/trending.pig

$ elastic-mapreduce --create --name 'pigflowtest2' --num-instances 1 --json pigflow.json --alive

        [ 
          { 
            "Name": "setup", 
            "ActionOnFailure": "CANCEL_AND_WAIT", 
            "HadoopJarStep": { 
              "Jar": "s3://us-east-1.elasticmapreduce/libs/script-runner/script-runner.jar",
              "Args": [ 
                 "s3://us-east-1.elasticmapreduce/libs/pig/0.3/fetch"
              ] 
            } 
          },
          { 
            "Name": "process 009", 
            "ActionOnFailure": "CANCEL_AND_WAIT", 
            "HadoopJarStep": { 
              "Jar": "/home/hadoop/lib/pig/pig-0.3-amzn.jar",
              "Args": [ 
                 "-p,piggybankjar=file:/home/hadoop/lib/pig/piggybank.jar", 
                 "-p,root_path=s3n://matpalm/trending/run1", 
		 "-p,input=009",
		 "-p,output=010",
                 "s3n://matpalm/trending/run1/scripts/trending.pig"
              ] 
            } 
          }
        ] 

and for none interactive (ie no --alive) we can run TERMINATE_JOB_FLOW instead CANCEL_AND_WAIT 
